{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import daft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as tt\n",
    "theano.config.warn.round=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "\n",
    "We will show how to estimate regression parameters using a simple linear model\n",
    "\n",
    "$$\n",
    "y \\sim ax + b\n",
    "$$\n",
    "\n",
    "We can restate the linear model $$y = ax + b + \\epsilon$$ as sampling from a probability distribution\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}(ax + b, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Now we can use `pymc` to estimate the parameters $a$, $b$ and $\\sigma$. We will assume the following priors\n",
    "\n",
    "$$\n",
    "a \\sim \\mathcal{N}(0, 100) \\\\\n",
    "b \\sim \\mathcal{N}(0, 100) \\\\\n",
    "\\sigma \\sim | \\mathcal{N(0, 1)} |\n",
    "$$\n",
    "\n",
    "Note: It may be useful to scale observed values to have zero mean and unit standard deviation to simplify choice of priors. However, you may need to back-transform the parameters to interpret the estimated values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plate diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "# Instantiate the PGM.\n",
    "pgm = daft.PGM(shape=[4.0, 3.0], origin=[-0.3, -0.7])\n",
    "\n",
    "# Hierarchical parameters.\n",
    "pgm.add_node(daft.Node(\"alpha\", r\"$\\alpha$\", 0.5, 2))\n",
    "pgm.add_node(daft.Node(\"beta\", r\"$\\beta$\", 1.5, 2))\n",
    "pgm.add_node(daft.Node(\"sigma\", r\"$\\sigma$\", 0, 0))\n",
    "\n",
    "# Deterministic variable.\n",
    "pgm.add_node(daft.Node(\"mu\", r\"$\\mu_n$\", 1, 1))\n",
    "\n",
    "# Data.\n",
    "pgm.add_node(daft.Node(\"x\", r\"$x_n$\", 2, 1, observed=True))\n",
    "pgm.add_node(daft.Node(\"y\", r\"$y_n$\", 1, 0, observed=True))\n",
    "\n",
    "# Add in the edges.\n",
    "pgm.add_edge(\"alpha\", \"mu\")\n",
    "pgm.add_edge(\"beta\", \"mu\")\n",
    "pgm.add_edge(\"x\", \"mu\")\n",
    "pgm.add_edge(\"mu\", \"y\")\n",
    "pgm.add_edge(\"sigma\", \"y\")\n",
    "\n",
    "# And a plate.\n",
    "pgm.add_plate(daft.Plate([0.5, -0.5, 2, 2], label=r\"$n = 1, \\cdots, N$\",\n",
    "    shift=-0.1))\n",
    "\n",
    "# Render and save.\n",
    "pgm.render()\n",
    "pgm.figure.savefig(\"lm.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up and fitting linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed data\n",
    "np.random.seed(123)\n",
    "n = 11\n",
    "_a = 6\n",
    "_b = 2\n",
    "x = np.linspace(0, 1, n)\n",
    "y = _a*x + _b + np.random.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 1000\n",
    "with pm.Model() as linreg:\n",
    "    a = pm.Normal('a', mu=0, sd=100)\n",
    "    b = pm.Normal('b', mu=0, sd=100)\n",
    "    sigma = pm.HalfNormal('sigma', sd=1)\n",
    "    \n",
    "    y_est = a*x + b     \n",
    "    likelihood = pm.Normal('y', mu=y_est, sd=sigma, observed=y)\n",
    "\n",
    "    trace = pm.sample(niter, random_seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace, varnames=['a', 'b'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=30, label='data')\n",
    "for a_, b_ in zip(trace['a'][-100:], trace['b'][-100:]):\n",
    "    plt.plot(x, a_*x + b_, c='gray', alpha=0.1)\n",
    "plt.plot(x, _a*x + _b, label='true regression line', lw=3., c='red')\n",
    "plt.legend(loc='best')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior predictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc = pm.sample_ppc(trace, samples=500, model=linreg, size=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot([np.mean(n) for n in ppc['y']], kde=True)\n",
    "plt.axvline(np.mean(y), color='red')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior_predictive_glm(\n",
    "    trace,\n",
    "    samples=50,\n",
    "    lm=lambda x, sample: sample['b'] + sample['a'] * x,\n",
    ")\n",
    "plt.scatter(x, y, s=30, label='data')\n",
    "plt.plot(x, _a*x + _b, label='true regression line', lw=3., c='red')\n",
    "plt.legend(loc='best')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the GLM module\n",
    "\n",
    "Many examples in [docs](pymc3 plot_posterior_predictive_glm lm example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    pm.glm.GLM.from_formula('y ~ x', df)\n",
    "    trace = pm.sample(2000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace, varnames=['Intercept', 'x'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "pm.plot_posterior_predictive_glm(trace, samples=50)\n",
    "plt.plot(x, _a*x + _b, label='true regression line', lw=3., c='red')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust linear regression\n",
    "\n",
    "If our data has outliers, we can perform a robust regression by modeling errors from a fatter tailed distribution than the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed data\n",
    "np.random.seed(123)\n",
    "n = 11\n",
    "_a = 6\n",
    "_b = 2\n",
    "x = np.linspace(0, 1, n)\n",
    "y = _a*x + _b + np.random.randn(n)\n",
    "y[5] *=10 # create outlier\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of outlier on linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 1000\n",
    "with pm.Model() as linreg:\n",
    "    a = pm.Normal('a', mu=0, sd=100)\n",
    "    b = pm.Normal('b', mu=0, sd=100)\n",
    "    sigma = pm.HalfNormal('sigma', sd=1)\n",
    "    \n",
    "    y_est = pm.Deterministic('mu', a*x + b)\n",
    "    y_obs = pm.Normal('y_obs', mu=y_est, sd=sigma, observed=y)\n",
    "\n",
    "    trace = pm.sample(niter, random_seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with linreg:\n",
    "    pp = pm.sample_posterior_predictive(trace, samples=100, vars=[a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=30, label='data')\n",
    "for a_, b_ in zip(pp['a'], pp['b']):\n",
    "    plt.plot(x, a_*x + b_, c='gray', alpha=0.1)\n",
    "plt.plot(x, _a*x + _b, label='true regression line', lw=3., c='red')\n",
    "plt.legend(loc='upper left')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a T-distribution for the errors for a more robust fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we sample [a, b] as a vector Î² using the `shape` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 1000\n",
    "with pm.Model() as robust_linreg:\n",
    "    beta = pm.Normal('beta', 0, 10, shape=2)\n",
    "    nu = pm.Exponential('nu', 1/len(x))\n",
    "    sigma = pm.HalfCauchy('sigma', beta=1)\n",
    "\n",
    "    y_est = beta[0] + beta[1]*x\n",
    "    y_obs = pm.StudentT('y_obs', mu=y_est, sd=sigma, nu=nu, observed=y)\n",
    "\n",
    "    trace = pm.sample(niter, random_seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with robust_linreg:\n",
    "    pp = pm.sample_posterior_predictive(trace, samples=100, vars=[beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=30, label='data')\n",
    "for a_, b_ in zip(pp['beta'][:,1], pp['beta'][:,0]):\n",
    "    plt.plot(x, a_*x + b_, c='gray', alpha=0.1)\n",
    "plt.plot(x, _a*x + _b, label='true regression line', lw=3., c='red')\n",
    "plt.legend(loc='upper left')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the GLM module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    pm.glm.GLM.from_formula('y ~ x', df, \n",
    "                            family=pm.glm.families.StudentT())\n",
    "    trace = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "pm.plot_posterior_predictive_glm(trace, samples=200)\n",
    "plt.plot(x, _a*x + _b, label='true regression line', lw=3., c='red')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Gelman's book has an example where the dose of a drug may be affected to the number of rat deaths in an experiment.\n",
    "\n",
    "| Dose (log g/ml) | # Rats | # Deaths |\n",
    "|-----------------|--------|----------|\n",
    "| -0.896          | 5      | 0        |\n",
    "| -0.296          | 5      | 1        |\n",
    "| -0.053          | 5      | 3        |\n",
    "| 0.727           | 5      | 5        |\n",
    "\n",
    "We will model the number of deaths as a random sample from a binomial distribution, where $n$ is the number of rats and $p$ the probability of a rat dying. We are given $n = 5$, but we believe that $p$ may be related to the drug dose $x$. As $x$ increases the number of rats dying seems to increase, and since $p$ is a probability, we use the following model:\n",
    "\n",
    "$$\n",
    "y \\sim \\text{Bin}(n, p) \\\\\n",
    "\\text{logit}(p) = \\alpha + \\beta x \\\\\n",
    "\\alpha \\sim \\mathcal{N}(0, 5) \\\\\n",
    "\\beta \\sim \\mathcal{N}(0, 10)\n",
    "$$\n",
    "\n",
    "where we set vague priors for $\\alpha$ and $\\beta$, the parameters for the logistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5 * np.ones(4)\n",
    "x = np.array([-0.896, -0.296, -0.053, 0.727])\n",
    "y = np.array([0, 1, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invlogit(x):\n",
    "    return tt.exp(x) / (1 + tt.exp(x))\n",
    "\n",
    "with pm.Model() as model:\n",
    "    alpha = pm.Normal('alpha', mu=0, sd=5)\n",
    "    beta = pm.Flat('beta')\n",
    "    \n",
    "    p = invlogit(alpha + beta*x)\n",
    "    y_obs = pm.Binomial('y_obs', n=n, p=p, observed=y)\n",
    "    \n",
    "    trace = pm.sample(niter, random_seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(a, b, xp): \n",
    "    return np.exp(a + b*xp)/(1 + np.exp(a + b*xp))\n",
    "\n",
    "with model:\n",
    "    pp = pm.sample_posterior_predictive(trace, samples=100, vars=[alpha, beta])\n",
    "\n",
    "xp = np.linspace(-1, 1, 100)\n",
    "a = trace['alpha'].mean()\n",
    "b = trace['beta'].mean()\n",
    "plt.plot(xp, logit(a, b, xp), c='red')\n",
    "\n",
    "for a_, b_ in zip(pp['alpha'], pp['beta']):\n",
    "    plt.plot(xp, logit(a_, b, xp), c='gray', alpha=0.2)\n",
    "\n",
    "plt.scatter(x, y/5, s=50);\n",
    "plt.xlabel('Log does of drug')\n",
    "plt.ylabel('Risk of death')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical model\n",
    "\n",
    "This uses the Gelman radon data set and is based off this [IPython notebook](http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/). Radon levels were measured in houses from all counties in several states. Here we want to know if the presence of a basement affects the level of radon, and if this is affected by which county the house is located in. \n",
    "\n",
    "![Radon](http://www.bestinspectionsllc.com/wp-content/uploads/2016/09/how-radon-enters-a-house.jpg)\n",
    "\n",
    "The data set provided is just for the state of Minnesota, which has 85 counties with 2 to 116 measurements per county. We only need 3 columns for this example `county`, `log_radon`, `floor`, where `floor=0` indicates that there is a basement.\n",
    "\n",
    "We will perform simple linear regression on log_radon as a function of county and floor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon = pd.read_csv('data/radon.csv')[['county', 'floor', 'log_radon']]\n",
    "radon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled model\n",
    "\n",
    "In the pooled model, we ignore the county infomraiton.\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}(a + bx, \\sigma^2)\n",
    "$$\n",
    "\n",
    "where $y$ is the log radon level, and $x$ an indicator variable for whether there is a basement or not.\n",
    "\n",
    "We make up some choices for the fairly uniformative priors as usual\n",
    "\n",
    "$$\n",
    "a \\sim \\mathcal{N}(\\mu, \\sigma_a^2) \\\\\n",
    "b \\sim \\mathcal{N}(\\mu, \\sigma_b^2) \\\\\n",
    "\\sigma \\sim \\text{Gamma(10, 1)}\n",
    "$$\n",
    "\n",
    "However, since the radon level varies by geographical location, it might make sense to include county information in the model. One way to do this is to build a separate regression model for each county, but the sample sizes for some counties may be too small for precise estimates. A compromise between the pooled and separate county models is to use a hierarchical model for *patial pooling* - the practical efffect of this is to shrink per county estimates towards the group mean, especially for counties with few observations.\n",
    "\n",
    "#### Hierarchical model\n",
    "\n",
    "With a hierarchical model, there is an $a_c$ and a $b_c$ for each county $c$ just as in the individual county model, but they are no longer independent but assumed to come from a common group distribution\n",
    "\n",
    "$$\n",
    "a_c \\sim \\mathcal{N}(\\mu_a, \\sigma_a^2) \\\\\n",
    "b_c \\sim \\mathcal{N}(\\mu_b, \\sigma_b^2)\n",
    "$$\n",
    "\n",
    "we further assume that the hyperparameters come from the following distributions\n",
    "\n",
    "$$\n",
    "\\mu_a \\sim \\mathcal{N}(0, 10^2) \\\\\n",
    "\\sigma_a \\sim \\text{|Cauchy(1)|} \\\\ \n",
    "\\mu_b \\sim \\mathcal{N}(0, 10^2) \\\\\n",
    "\\sigma_b \\sim \\text{|Cauchy(1)|} \\\\ \n",
    "$$\n",
    "\n",
    "The variance for observations does not change, so the model for the radon level is\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}(a_c + b_c x, \\sigma^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 1000\n",
    "with pm.Model() as pl:\n",
    "    # County hyperpriors\n",
    "    mu_a = pm.Normal('mu_a', mu=0, sd=10)\n",
    "    sigma_a = pm.HalfCauchy('sigma_a', beta=1)\n",
    "    mu_b = pm.Normal('mu_b', mu=0, sd=10)\n",
    "    sigma_b = pm.HalfCauchy('sigma_b', beta=1)\n",
    "    \n",
    "    # County slopes and intercepts\n",
    "    a = pm.Normal('slope', mu=mu_a, sd=sigma_a)\n",
    "    b = pm.Normal('intercept', mu=mu_b, sd=sigma_b)\n",
    "    \n",
    "    # Houseehold errors\n",
    "    sigma = pm.Gamma(\"sigma\", alpha=10, beta=1)\n",
    "    \n",
    "    # Model prediction of radon level\n",
    "    mu = a + b * radon.floor.values\n",
    "    \n",
    "    # Data likelihood\n",
    "    y = pm.Normal('y', mu=mu, sd=sigma, observed=radon.log_radon)\n",
    "\n",
    "    pl_trace = pm.sample(niter, tune=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(pl_trace, varnames=['slope', 'intercept'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = pd.Categorical(radon['county']).codes\n",
    "\n",
    "niter = 1000\n",
    "with pm.Model() as hm:\n",
    "    # County hyperpriors\n",
    "    mu_a = pm.Normal('mu_a', mu=0, sd=10)\n",
    "    sigma_a = pm.HalfCauchy('sigma_a', beta=1)\n",
    "    mu_b = pm.Normal('mu_b', mu=0, sd=10)\n",
    "    sigma_b = pm.HalfCauchy('sigma_b', beta=1)\n",
    "    \n",
    "    # County slopes and intercepts\n",
    "    a = pm.Normal('slope', mu=mu_a, sd=sigma_a, shape=len(set(county)))\n",
    "    b = pm.Normal('intercept', mu=mu_b, sd=sigma_b, shape=len(set(county)))\n",
    "    \n",
    "    # Houseehold errors\n",
    "    sigma = pm.Gamma(\"sigma\", alpha=10, beta=1)\n",
    "    \n",
    "    # Model prediction of radon level\n",
    "    mu = a[county] + b[county] * radon.floor.values\n",
    "    \n",
    "    # Data likelihood\n",
    "    y = pm.Normal('y', mu=mu, sd=sigma, observed=radon.log_radon)\n",
    "\n",
    "    hm_trace = pm.sample(niter, tune=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the length of the credible interval with the number of observations for each county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.Categorical(radon['county'])\n",
    "pd.DataFrame(dict(\n",
    "    code=range(len(cat.categories)),\n",
    "    n=pd.value_counts(pd.Categorical(radon['county']), sort=False), \n",
    ")).sort_values('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm.forestplot(hm_trace, varnames=['slope', 'intercept'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loo = pm.compare({'pooled': pl_trace, 'hierarchical': hm_trace}, ic='LOO')\n",
    "df_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_waic = pm.compare({'pooled': pl_trace, 'hierarchical': hm_trace}, ic='WAIC')\n",
    "df_waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
