{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyMC3\n",
    "\n",
    "PyMC3 is a Python package for doing MCMC using a variety of samplers, including Metropolis, Slice and Hamiltonian Monte Carlo. See [Probabilistic Programming in Python using PyMC](http://arxiv.org/abs/1507.08050) for a description. The GitHub [site](https://github.com/pymc-devs/pymc3) also has many examples and links for further exploration.\n",
    "\n",
    "Note: [PyMC4](https://github.com/pymc-devs/pymc4) is based on TensorFlow rather than Theano but will have a similar API so everyghitn learned should be transferable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "! pip install --quiet arviz\n",
    "! pip install --quiet pymc3\n",
    "! pip install --quiet daft\n",
    "! pip install --quiet seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "! conda install --yes --quiet mkl-service\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other resources**\n",
    "\n",
    "Some examples are adapted from:\n",
    "\n",
    "- [Probabilistic Programming & Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)\n",
    "- [MCMC tutorial series](https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import scipy.stats as stats\n",
    "import daft\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "theano.config.warn.round=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions in pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join([d for d in dir(pm.distributions) if d[0].isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pm.Normal.dist(mu=0, sd=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.random(size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.logp(0).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom distributions\n",
    "\n",
    "The pymc3 algorithms generally only work with the log probability of a distribution. Hence it is easy to define custom distributions to use in your models by providing a `logp` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp(x, μ=0, σ=1):\n",
    "    \"\"\"Normal distribtuion.\"\"\"\n",
    "    return -0.5*np.log(2*np.pi) - np.log(σ) - (x-μ)**2/(2*σ**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pm.DensityDist.dist(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.logp(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Estimating coin bias\n",
    "\n",
    "We start with the simplest model - that of determining the bias of a coin from observed outcomes.\n",
    "\n",
    "Here the prior is a beta distribution with paramters $a$ and $b$, the likelihood assumes that the number of heads follows a binomial distribution with parameters $n$ and $p$, and we wish to estimate the posterior distribution of $\\theta = p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior = Beta distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = [(0.5, 0.5), (1,1), (1,10), (5,5), (10, 1), (10,10)]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "for i, (a, b) in enumerate(pars):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    θ = np.linspace(0, 1, 100)\n",
    "    dens = stats.beta(a, b).pdf(θ)\n",
    "    ax.plot(θ, dens)\n",
    "    ax.set_title('a=%.1f, b=%.1f' % (a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior = bionmial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "n = 10\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "for i, p in enumerate(pars):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    θ = np.arange(0, n+1)\n",
    "    dens = stats.binom(n, p).pmf(θ)\n",
    "    ax.stem(θ, dens)\n",
    "    ax.set_title('n=%.1f, p=%.1f' % (n, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "heads = 61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 10, 10\n",
    "prior = stats.beta(a, b)\n",
    "post = stats.beta(heads+a, n-heads+b)\n",
    "ci = post.interval(0.95)\n",
    "\n",
    "xs = np.linspace(0, 1, 100)\n",
    "plt.plot(prior.pdf(xs), label='Prior')\n",
    "plt.plot(post.pdf(xs), c='green', label='Posterior')\n",
    "plt.axvline(100*heads/n, c='red', alpha=0.4, label='MLE')\n",
    "plt.xlim([0, 100])\n",
    "plt.axhline(0.3, ci[0], ci[1], c='black', linewidth=2, label='95% CI');\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = daft.PGM(shape=[2.5, 3.0], origin=[0, -0.5])\n",
    "\n",
    "pgm.add_node(daft.Node(\"alpha\", r\"$\\alpha$\", 0.5, 2, fixed=True))\n",
    "pgm.add_node(daft.Node(\"beta\", r\"$\\beta$\", 1.5, 2, fixed=True))\n",
    "pgm.add_node(daft.Node(\"p\", r\"$p$\", 1, 1))\n",
    "pgm.add_node(daft.Node(\"n\", r\"$n$\", 2, 0, fixed=True))\n",
    "pgm.add_node(daft.Node(\"y\", r\"$y$\", 1, 0, observed=True))\n",
    "\n",
    "pgm.add_edge(\"alpha\", \"p\")\n",
    "pgm.add_edge(\"beta\", \"p\")\n",
    "pgm.add_edge(\"n\", \"y\")\n",
    "pgm.add_edge(\"p\", \"y\")\n",
    "\n",
    "pgm.render()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model context\n",
    "\n",
    "When you specify a model, you are adding nodes to a computation graph. When executed, the graph is compiled via Theno. Hence, `pymc3` uses the Model context manager to automatically add new nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 2000\n",
    "with pm.Model() as coin_context:\n",
    "    p = pm.Beta('p', alpha=2, beta=2)\n",
    "    y = pm.Binomial('y', n=n, p=p, observed=heads)\n",
    "    trace = pm.sample(niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformed prior variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_context.free_RVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_context.deterministics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables in likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_context.observed_RVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theano\n",
    "\n",
    "Theano builds functions as mathematical expression graphs and compiles them into C for actual computation, making use of GPU resources where available.\n",
    "\n",
    "Performing calculations in Theano generally follows the following 3 steps (from official docs):\n",
    "\n",
    "- declare variables (a,b) and give their types\n",
    "- build expressions for how to put those variables together\n",
    "- compile expression graphs to functions in order to use them for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as tt\n",
    "theano.config.compute_test_value = \"off\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part builds symbolic expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tt.iscalar('a')\n",
    "b = tt.iscalar('x')\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step compiles a function whose inputs are [a, b] and outputs are [c]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = theano.function([a, b], [c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within a model context, \n",
    "\n",
    "- when you add an unbounded variable, it is defined as a Theano variable and added to the prior part of the log posterior function\n",
    "- when you add a bounded variable, a transformed version is defined as a Theano variable and and added to the log posterior function\n",
    "    - The inverse transformation is used to define the original variable - this is a deterministic variable\n",
    "- when you add an observed variable bound to data, the data is added to the likelihood part of the log posterior\n",
    "\n",
    "See [PyMC3 and Theano](https://docs.pymc.io/PyMC3_and_Theano.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "help(pm.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying sampler (step) and multiple chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with coin_context:\n",
    "    step = pm.Metropolis()\n",
    "    t = pm.sample(niter, step=step, chains=8, random_seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samplers available\n",
    "\n",
    "For continuous distributions, it is hard to beat NUTS and hence this is the default. To learn more, see [A Conceptual Introduction to Hamiltonian Monte Carlo](https://arxiv.org/pdf/1701.02434.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(m for m in dir(pm.step_methods) if m[0].isupper()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the sampler will be automatically selected based on the type of the variable (discrete or continuous), but there are many samples that you can explicitly specify if you want to learn more about them or understand why an alternative would be better than the default for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 2000\n",
    "with pm.Model() as normal_context:\n",
    "    mu = pm.Normal('mu', mu=0, sd=100)\n",
    "    sd = pm.HalfCauchy('sd', beta=2)\n",
    "    y = pm.Normal('y', mu=mu, sd=sd, observed=xs)\n",
    "    \n",
    "    step1 = pm.Slice(vars=mu)\n",
    "    step2 = pm.Metropolis(vars=sd)\n",
    "    \n",
    "    t = pm.sample(niter, step=[step1, step2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(t)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as m:\n",
    "    p = pm.Beta('p', alpha=2, beta=2)\n",
    "    y = pm.Binomial('y', n=n, p=p, observed=heads)\n",
    "    θ = pm.find_MAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "θ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting values from the trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the information about the posterior is in the trace, and it also provides statistics about the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "help(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.stat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(trace.get_sampler_stats('model_logp'))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = trace.get_values('p')\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace['p'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to `pandas` data frame for downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pm.trace_to_dataframe(trace)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(trace['p'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocorrelation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.autocorrplot(trace, varnames=['p'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate effective sample size\n",
    "\n",
    "$$\n",
    "\\hat{n}_{eff} = \\frac{mn}{1 + 2 \\sum_{t=1}^T \\hat{\\rho}_t}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of chains, $n$ the number of steps per chain, $T$ the time when the autocorrelation first becomes negative, and $\\hat{\\rho}_t$ the autocorrelation at lag $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.effective_n(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate convergence\n",
    "\n",
    "[Model checking and diagnostics](https://pymc-devs.github.io/pymc/modelchecking.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gelman-Rubin\n",
    "\n",
    "$$\n",
    "\\hat{R} = \\sqrt{\\frac{\\hat{\\text{Var}}(\\theta | y)}{W}}\n",
    "$$\n",
    "\n",
    "where $W$ is the within-chain variance and the numeratro is the posterior variance estimate for the pooled traces.  Values greater than one indicate that one or more chains have not yet converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.gelman_rubin(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geweke\n",
    "\n",
    "Compares mean and variance of initial with later segments of a trace for a parameter. Should have absolute value less than 1 at convergence.\n",
    "\n",
    "$$\n",
    "z = \\frac{\\bar{\\theta}_a - \\bar{\\theta}_b}{\\sqrt{Var(\\theta_a) + Var(\\theta_b)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pm.geweke(trace['p'])[:,1], 'o')\n",
    "plt.axhline(1, c='red')\n",
    "plt.axhline(-1, c='red')\n",
    "plt.gca().margins(0.05)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Textual summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace, varnames=['p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace, varnames=['p'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(trace)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior predictive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with coin_context:\n",
    "    ps = pm.sample_prior_predictive(samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ps['y'])\n",
    "plt.axvline(heads, c='red')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior predictive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with coin_context:\n",
    "    ps = pm.sample_posterior_predictive(trace, samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ps['y'])\n",
    "plt.axvline(heads, c='red')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.save_trace(trace, 'my_trace', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to re-initialize the model when reloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as my_trace:\n",
    "    p = pm.Beta('p', alpha=2, beta=2)\n",
    "    y = pm.Binomial('y', n=n, p=p, observed=heads)\n",
    "    tr = pm.load_trace('my_trace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is probably a good practice to make model reuse convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    with pm.Model() as m:\n",
    "        p = pm.Beta('p', alpha=2, beta=2)\n",
    "        y = pm.Binomial('y', n=n, p=p, observed=heads)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m:\n",
    "    tr1 = pm.load_trace('my_trace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(tr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating parameters of a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.normal(5, 2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from prior\n",
    "\n",
    "Just omit the `observed=` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as prior_context:\n",
    "    sigma = pm.Gamma('sigma', alpha=2.0, beta=1.0)\n",
    "    mu = pm.Normal('mu', mu=0, sd=sigma)\n",
    "    trace = pm.sample(niter, step=pm.Metropolis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace, varnames=['mu', 'sigma'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 2000\n",
    "with pm.Model() as normal_context:\n",
    "    mu = pm.Normal('mu', mu=0, sd=100)\n",
    "    sd = pm.HalfCauchy('sd', beta=2)\n",
    "    y = pm.Normal('y', mu=mu, sd=sd, observed=xs)\n",
    "    trace = pm.sample(niter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Highest Posterior Density (Credible intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpd = pm.hpd(trace['mu'])\n",
    "hpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pm.traceplot(trace, varnames=['mu'],)\n",
    "\n",
    "ymin, ymax = ax[0,0].get_ylim()\n",
    "y = ymin + 0.05*(ymax-ymin)\n",
    "ax[0, 0].plot(hpd, [y,y], c='red')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating goodness-of-fit\n",
    "\n",
    "WAIC is an approximation to the out-of-sample error and can be used for model comparison. Likelihood is dependent on model complexity and should not be used for model comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with normal_context:\n",
    "    print(pm.loo(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WAIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with normal_context:\n",
    "    print(pm.waic(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models\n",
    "\n",
    "Use precomputed models for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = az.load_arviz_data(\"non_centered_eight\")\n",
    "data2 = az.load_arviz_data(\"centered_eight\")\n",
    "compare_dict = {\"non centered\": data1, \"centered\": data2}\n",
    "az.compare(compare_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.compare(compare_dict, ic='LOO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a custom likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp(x, μ=0, σ=1):\n",
    "    \"\"\"Normal distribtuion.\"\"\"\n",
    "    return -0.5*np.log(2*np.pi) - np.log(σ) - (x-μ)**2/(2*σ**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as prior_context:\n",
    "    mu = pm.Normal('mu', mu=0, sd=100)\n",
    "    sd = pm.HalfCauchy('sd', beta=2)\n",
    "    y = pm.DensityDist('y', logp, observed=dict(x=xs, μ=mu, σ=sd))\n",
    "    custom_trace = pm.sample(niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.trace_to_dataframe(custom_trace).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational methods available\n",
    "\n",
    "To use a variational method, use `pm.fit` instead of `pm.sample`. We'll see examples of usage in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(m for m in dir(pm.variational) if m[0].isupper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
